{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Langchain\n",
    "\n",
    "Langchain allows you to interact with your models in a standardized way and lets your easily compose components from langchain to fulfill your specific usecase"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.1)\n",
      "Collecting openai==0.28.0\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai==0.28.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai==0.28.0) (4.66.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai==0.28.0) (3.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai==0.28.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai==0.28.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai==0.28.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai==0.28.0) (2024.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai==0.28.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai==0.28.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai==0.28.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai==0.28.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai==0.28.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai==0.28.0) (4.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gulsensabak\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->openai==0.28.0) (0.4.5)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.40.0\n",
      "    Uninstalling openai-1.40.0:\n",
      "      Successfully uninstalled openai-1.40.0\n",
      "Successfully installed openai-0.28.0\n"
     ]
    }
   ],
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "source": [
    "!pip install python-dotenv\n",
    "!pip install openai==0.28.0"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 4,
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 5,
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(input):\n",
    "    messages = [{\"role\": \"user\", \"content\": input}]\n",
    "    # create function make a request to the openai API to create an answer\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = messages,\n",
    "        temperature = 0,\n",
    "    )\n",
    "    # response object is a complex object\n",
    "    # we can access the actual message that we can receive from openai in that way\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "source": [
    "output = chat(\"what is the capital of France?\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 11,
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show the response object\n",
    "def wholechat(input):\n",
    "    messages = [{\"role\": \"user\", \"content\": input}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = messages,\n",
    "        temperature = 0,\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-9vheVX2KCsDe1GNdha5ZwGydIEXAI at 0x1fa297ce250> JSON: {\n",
       "  \"id\": \"chatcmpl-9vheVX2KCsDe1GNdha5ZwGydIEXAI\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1723539523,\n",
       "  \"model\": \"gpt-3.5-turbo-0125\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"LLMs (Large Language Models) and AI (Artificial Intelligence) are closely connected as LLMs are a type of AI technology that focuses on natural language processing and generation. LLMs are advanced AI models that are trained on vast amounts of text data to understand and generate human-like language. They are used in various applications such as chatbots, language translation, content generation, and more. AI technologies like LLMs have significantly advanced the field of natural language processing and have enabled the development of more sophisticated language-based applications.\",\n",
       "        \"refusal\": null\n",
       "      },\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 17,\n",
       "    \"completion_tokens\": 107,\n",
       "    \"total_tokens\": 124\n",
       "  },\n",
       "  \"system_fingerprint\": null\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "source": [
    "output = wholechat(\"what is the connection between llms and ai?\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Be very funny when answering questions\n",
      "Question = what is the capital of france?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris, where the croissants are flakier than my excuses for not going to the gym.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "source": [
    "# We may want an LLM to behave in a specific way\n",
    "\n",
    "question = \"what is the capital of france?\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Be very funny when answering questions\n",
    "Question = {question}\n",
    "\"\"\".format(question= question)\n",
    "\n",
    "print(prompt)\n",
    "chat(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
