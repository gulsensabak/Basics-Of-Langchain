{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 2,
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gulsensabak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\gulsensabak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n{\\n    \"sentiment\": \"positive\",\\n    \"subject\": \"food\"\\n}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "source": [
    "template = \"\"\" \n",
    "Interprete the text and evaluate the text.\n",
    "sentiment: is the text in a positive, neutral or negative sentiment?\n",
    "subject: What subject is the text about? Use exactly one word.\n",
    "\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "sentiment\n",
    "subject\n",
    "\n",
    "text: {input}\n",
    "\"\"\"\n",
    "\n",
    "llm = OpenAI(temperature = 0)\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "chain.predict(input= \"I ordered Pizza Salami and it was awesome!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chains\n",
    "\n",
    "Sometimes you want to pass the output from one model to another model. This can be done with different Sequential Chains\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 12,
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"\"\"\n",
    "You are a helpful bot that creates a 'thank you' response text.\n",
    "If customers are unsatisfied, offer them a real world assistant to talk to.\n",
    "You will get a sentiment and subject as into and evaluate.\n",
    "\n",
    "text: {input}\n",
    "\"\"\"\n",
    "review_template = PromptTemplate(input_variables=[\"input\"], template = response_template)\n",
    "\n",
    "review_chain = LLMChain(llm = llm, prompt=review_template)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "{\n",
      "    \"sentiment\": \"negative\",\n",
      "    \"subject\": \"food\"\n",
      "}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "I'm sorry to hear that you had a negative experience with our food. We strive to provide the best quality and taste for our customers. If you would like to discuss your concerns further, please don't hesitate to reach out to one of our real world assistants who would be happy to assist you. Thank you for bringing this to our attention.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nI'm sorry to hear that you had a negative experience with our food. We strive to provide the best quality and taste for our customers. If you would like to discuss your concerns further, please don't hesitate to reach out to one of our real world assistants who would be happy to assist you. Thank you for bringing this to our attention.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "# This creates a chain. review_chain uses the output of chain as input\n",
    "overall_chain = SimpleSequentialChain(chains= [chain, review_chain], verbose = True)\n",
    "\n",
    "# run function gives the impulse\n",
    "overall_chain.run(input= \"I ordered Pizza Salami and was awful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains can be more complex and not all sequential chains will be as simple as passing a single string as an argument and getting a single string as output for all steps in the chain"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gulsensabak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dish_name': 'Pizza Salami',\n",
       " 'experience': 'It was awful!',\n",
       " 'review': '\\n\\nI recently ordered a Pizza Salami from your restaurant and I have to say, it was an awful experience. I was really looking forward to enjoying a delicious pizza, but unfortunately, that was not the case.\\n\\nFirst of all, the pizza arrived almost an hour late. I understand that sometimes there can be delays, but an hour is just unacceptable. By the time it arrived, I was already starving and my excitement for the pizza had diminished.\\n\\nWhen I opened the box, I was disappointed to see that the pizza was not cooked properly. The crust was soggy and the toppings were barely cooked. It seemed like it was rushed and not given enough time in the oven. The salami was also not evenly distributed and some slices were burnt while others were barely there.\\n\\nThe taste of the pizza was also not up to par. The sauce was bland and lacked any flavor. The cheese was not melted properly and had a strange texture. The salami itself was also not of good quality and had a strange aftertaste.\\n\\nOverall, my experience with the Pizza Salami from your restaurant was extremely disappointing. I was expecting a delicious and satisfying meal, but instead, I was left with a poorly cooked and unappetizing pizza. I hope you can take this feedback into',\n",
       " 'comment': '\\n\\nThank you for taking the time to share your experience with us. We are deeply sorry to hear that your recent order of Pizza Salami was not up to your expectations. We strive to provide our customers with delicious and high-quality food, and it is clear that we fell short in this instance.\\n\\nWe apologize for the delay in your order and understand how frustrating it can be to wait for your food. We will be addressing this issue with our kitchen staff to ensure that orders are delivered in a timely manner.\\n\\nWe are also disappointed to hear that the pizza was not cooked properly and that the toppings were not evenly distributed. This is not the standard of quality that we hold ourselves to and we will be working with our chefs to ensure that all pizzas are cooked to perfection.\\n\\nWe appreciate your feedback on the taste of the pizza as well. We will be reviewing our recipe and ingredients to ensure that our Pizza Salami has the delicious flavor that our customers expect.\\n\\nWe value your patronage and hope that you will give us another chance to make it up to you. We would love the opportunity to serve you a properly cooked and delicious Pizza Salami. Please reach out to us so we can make things right. Thank you again for your feedback.',\n",
       " 'summary': ' consideration and improve the quality of your food and service.\\n\\nDisappointing experience with Pizza Salami from restaurant due to late delivery, poorly cooked crust and toppings, bland sauce, and low quality salami.',\n",
       " 'german_translation': '\\n\\nBerücksichtigung und Verbesserung der Qualität von Essen und Service. Enttäuschende Erfahrung mit Pizza Salami aus dem Restaurant aufgrund verspäteter Lieferung, schlecht gebackenem Teig und Belag, fade Soße und minderwertiger Salami.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# This is an LLMChain to write a review given a dish name and the experience\n",
    "prompt_review = PromptTemplate.from_template(template= \"you ordered {dish_name} and your experience was {experience}. Write a review: \")\n",
    "chain_review = LLMChain(llm = llm, prompt = prompt_review, output_key = \"review\")\n",
    "\n",
    "# This is an LLMChain to write a follow-up comment given the restaurant review\n",
    "prompt_comment = PromptTemplate.from_template(template = \"Given the restaurant review: {review}, write a follow-up comment: \")\n",
    "chain_comment = LLMChain(llm = llm, prompt = prompt_comment, output_key=\"comment\")\n",
    "\n",
    "# This is an LLMChain to summerize a review\n",
    "prompt_summary = PromptTemplate.from_template(template=\"Summerize the review in one short sentence: \\n\\n {review}\")\n",
    "chain_summary = LLMChain(llm = llm, prompt = prompt_summary, output_key=\"summary\")\n",
    "\n",
    "# This is an LLMChain to translate a summary into German\n",
    "prompt_translation = PromptTemplate.from_template(template = \"Translate the summary to German:  \\n\\n {summary}\")\n",
    "chain_translation = LLMChain(llm = llm, prompt = prompt_translation, output_key=\"german_translation\")\n",
    "\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains = [chain_review, chain_comment, chain_summary, chain_translation],\n",
    "    input_variables = [\"dish_name\", \"experience\"],\n",
    "    output_variables = [\"review\", \"comment\", \"summary\", \"german_translation\"],\n",
    ")\n",
    "overall_chain({\"dish_name\": \"Pizza Salami\", \"experience\": \"It was awful!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of chaining multiple chains together we can also use an LLM to decide which follow up chain is being used"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 18,
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "positive_template = \"\"\"You are an AI that focuses on the positive side of things. \\\n",
    "Whenever you analyze a text, you look for the positive aspects and highlight them. \\\n",
    "Here is the text:\n",
    "{input}\"\"\"\n",
    "\n",
    "neutral_template = \"\"\"You are an AI that has a neutral perspective. You just provide a balanced analysis of the text, \\\n",
    "not favoring any positive or negative aspects. Here is the text:\n",
    "{input}\"\"\"\n",
    "\n",
    "negative_template = \"\"\"You are an AI that is designed to find the negative aspects in a text. \\\n",
    "You analyze a text and show the potential downsides. Here is the text:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': LLMChain(prompt=PromptTemplate(input_variables=['input'], template='You are an AI that focuses on the positive side of things. Whenever you analyze a text, you look for the positive aspects and highlight them. Here is the text:\\n{input}'), llm=OpenAI(client=<class 'openai.api_resources.completion.Completion'>, openai_api_key='sk-proj-fx3O5xkscOREFACfsOP5T3BlbkFJQi2FTasash9NnwIveJKV', openai_proxy='')),\n",
       " 'neutral': LLMChain(prompt=PromptTemplate(input_variables=['input'], template='You are an AI that has a neutral perspective. You just provide a balanced analysis of the text, not favoring any positive or negative aspects. Here is the text:\\n{input}'), llm=OpenAI(client=<class 'openai.api_resources.completion.Completion'>, openai_api_key='sk-proj-fx3O5xkscOREFACfsOP5T3BlbkFJQi2FTasash9NnwIveJKV', openai_proxy='')),\n",
       " 'negative': LLMChain(prompt=PromptTemplate(input_variables=['input'], template='You are an AI that is designed to find the negative aspects in a text. You analyze a text and show the potential downsides. Here is the text:\\n{input}'), llm=OpenAI(client=<class 'openai.api_resources.completion.Completion'>, openai_api_key='sk-proj-fx3O5xkscOREFACfsOP5T3BlbkFJQi2FTasash9NnwIveJKV', openai_proxy=''))}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"positive\",\n",
    "        \"description\": \"Good for analyzing positive sentiments\",\n",
    "        \"prompt_template\": positive_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"neutral\",\n",
    "        \"description\": \"Good for neutral positive sentiments\",\n",
    "        \"prompt_template\": neutral_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"negative\",\n",
    "        \"description\": \"Good for analyzing negative sentiments\",\n",
    "        \"prompt_template\": negative_template\n",
    "    }\n",
    "]\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "destination_chains = {}\n",
    "\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm = llm, prompt = prompt)\n",
    "    destination_chains[name] = chain\n",
    "destination_chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations = destinations_str)\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template = router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    # route the output of llm to one of the our llms\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(router_chain= router_chain,\n",
    "                         destination_chains=destination_chains,\n",
    "                         default_chain=destination_chains['neutral'], verbose = True)\n",
    "chain.run(\"I orered Pizza Salami for 9.95$ and it was awesome!\")"
   ]
=======
   "source": []
>>>>>>> 3cbe73e15887ad9c42426e3c460f9c8ffc7eb9db
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
